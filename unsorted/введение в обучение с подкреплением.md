---
tags:
  - робошкола
---
Агент знает что делать, но не знает как
Цель задачи - выучить некоторое поведение

Агент выполняет действие, действие производит влияние на среду, среда меняется, рассчитывается функция вознаграждения

Марковский процесс принятия решений
- Множество состояний среды и агента S
- множество возможных действий агента A
- вероятность перехода из состояния s в s' в момент t ($P_a(s, s')$)
- функция награды $R_a(s,s')$

Процесс обладает марковским свойством, если следующее состояние зависит только от текущего

В обучении с подкреплением добавляется *стратегия*
Стратегия определяет, какой действие предпринять в заданном состоянии
- стохастическая стратегия (набор вероятностей действий) $pi: A \times S \rightarrow [0, 1]$
- детерминированная стратегия $pi: S \rightarrow A$
# Оценка эффективности стратегии
1) Сумма всех наград
2) Сумма наград за конечное время (не всегда понятно, сколько шагов разрешено)
3) Сумма наград с учётом дисконтирования $R_t=\sum^{\infty}_{i=t}{\gamma^{i-t}r_i}$
	если $\gamma=1$, то это первый случай

цель агента - максимизация суммарного вознаграждения
### V-функция
Функция полезности состояния $V_\pi(s)$ - ожидаемое вознаграждение начиная с состояния s и следования стратегии $\pi$
### Q-функция
Функция полезности действия $Q_\pi(s, a)$ - ожидаемое вознаграждение при применении в состоянии s действия a и в дальнейшем следования стратегии $\pi$

Все оптимальные стратегии имеют одинаковую Q-функцию, которая удовлетворяет уравнению Беллмана
## Методы обучения с подкреплением
- основанные на модели
	модель дана или попытка выучить модель
- методы, не требующие построения модели
	- прямая оптимизация стратегии
	- оценка Q-функции
## Имитационное обучение
Есть образец, согласно которому обучается модель
# Симуляция
Для того, чтобы обучать агента, обучение разбивают на попытки.

Для того, чтобы использовать Q-таблицу, её надо сначала заполнить. Для этого все действия изначально имеют случайный характер.

В процессе обучения при заполнении Q-таблицы стоит изменять значения постепенно
$Q=(1-\alpha)Q_{old}+\alpha Q_{new}$
$Q_{new} = r_t+\gamma*max_aQ(s_{t+1}, \alpha)$
где $r_t$ - награда за текущий шаг
$\gamma$ - коэффициент дисконтирования

Вместо Q-таблицы в качестве аппроксиматора сейчас чаще используют нейронную сеть
Это называется Deep Q Learning (DQM)
Каждые 10 этапов копируем нейросеть, которая справилась лучше всего, как эталонную

Если для получения хоть какой-то награды необходимо выполнить сложную последовательность действий, то это задача с разреженной наградой.
Для этого стоит либо разбить задачу на подзадачи, либо решить обратную задачу.
В обратной задаче начальные условия максимально близки к целевому положению. Со временем начальные условия изменяются всё дальше от цели.
Изменение принципа награждения так же помогает эффективно учиться.

# Обучение читерством (Learning by cheating)
Сначала создать агента у которого много данных и которому проще решать задачу. Им сгенерировать множество правильных решений (расширить экспертную выборку). Затем, создать обычного агента, который бы учился на данных агента-читера.