# Разница между функциями активации: преимущества и недостатки

## Сигмоида (Sigmoid)
- **Формула**:
	$$\sigma(x) = \frac{1}{1 + e^{-x}}$$
	![Sigmoid](Sigmoid.png)
- **Преимущества**:
    - Простая интерпретация, значения на выходе находятся в диапазоне от 0 до 1, что можно интерпретировать как вероятности.
- **Недостатки**:
    - Проблема затухания градиента: градиент сигмоиды близок к нулю вне диапазона (-5, 5), что может замедлить обучение глубоких сетей из-за его затухания.

## Гиперболический тангенс (Tanh)
- **Формула**:
	$$\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$
	![Tanh](Tanh.png)
- **Преимущества**:
    - Значения на выходе находятся в диапазоне от -1 до 1, что уменьшает вероятность затухания градиента по сравнению с сигмоидой.
    - Симметрична относительно нуля, что может помочь ускорить сходимость.
- **Недостатки**:
    - По-прежнему подвержена проблеме затухания градиента.

## ReLU (Rectified Linear Unit)
- **Формула**: 
	$$\text{ReLU}(x) = \max(0, x)$$
	![ReLU](ReLU.png)
- **Преимущества**:
    - Быстрая сходимость: ReLU обеспечивает быструю сходимость обучения, так как она не имеет наклона в отрицательной области и не вызывает затухания градиента.
    - Способствует разреженности активаций, что может помочь в предотвращении переобучения.
- **Недостатки**:
    - Проблема "умерших нейронов" (dying ReLU): нейрон может перейти в состояние "мертвого", при котором он продолжает выводить ноль.

### Leaky ReLU
- **Формула**: $$
\text{LeakyReLU}(x) = \max(\alpha x, x)
$$
	Где $\alpha$ - маленькое положительное число, например, 0.01.
- **Преимущества**:
    - Решает проблему "умерших нейронов" путем разрешения отрицательных значений с небольшим уклоном.
- **Недостатки**:
    - Может быть несколько более сложным в обучении, чем обычный ReLU, так как есть дополнительный гиперпараметр $\alpha$
### Parametric ReLU
- **Формула**: $$
    \text{PReLU}(x) = \max(\alpha x, x)
    $$
    Где $\alpha$ - параметр, вычисляемый во время обучения.
- Подходит для задач, где требуется тщательная оптимизация и настройка модели.
- Особенно полезна, когда у вас есть большие объемы данных и ресурсы для экспериментов с различными конфигурациями модели.
- PReLU может обеспечить лучшую производительность по сравнению с Leaky ReLU, особенно в задачах, связанных с компьютерным зрением и обработкой изображений.
### Замечания к ReLU
- Когда вы используете Leaky ReLU и PReLU в своих моделях, важно учитывать несколько ключевых моментов:
	- **Инициализация весов**: Эти функции активации могут быть чувствительны к начальной инициализации весов. Неправильная инициализация может привести к тому, что большая часть нейронов окажется в "мертвом" состоянии.
	- **Регуляризация**: Особенно в случае PReLU, где параметры функции активации являются обучаемыми, важно следить за переобучением. Использование методов регуляризации, таких как dropout или L1/L2 регуляризация, может быть полезным.
	- **Настройка параметров**: В Leaky ReLU необходимо настроить параметр alpha (коэффициент утечки). Этот параметр обычно находится в диапазоне от 0.01 до 0.03, но оптимальное значение может варьироваться в зависимости от задачи.
	- **Мониторинг обучения**: При использовании этих функций активации важно тщательно мониторить процесс обучения, чтобы убедиться, что нейронная сеть эффективно обучается и не возникают проблемы с исчезающим или взрывающимся градиентом.
	- **Тестирование и экспериментирование**: Важно проводить эксперименты с различными конфигурациями и параметрами, чтобы определить, какая функция активации работает лучше всего для вашей конкретной задачи. Может потребоваться тестирование с различными значениями alpha для Leaky ReLU или сравнение производительности между Leaky ReLU и PReLU.
	- **Комбинирование с другими слоями**: Leaky ReLU и PReLU могут быть эффективно использованы в комбинации с другими типами слоев, включая сверточные слои (в CNN) и рекуррентные слои (в RNN).

- Важно отметить, что как Leaky ReLU, так и PReLU требуют дополнительных экспериментов и тестирования, чтобы определить их эффективность в конкретной задаче. Также они могут быть более подвержены переобучению по сравнению с обычной ReLU, особенно в небольших или менее сложных сетях.
## ELU (Exponential Linear Unit)
- **Формула**:
	$$
	\begin{equation}
	f(x) = 
	\begin{cases}
	  x & \text{если } x \geq 0 \\
	  \alpha(e^x - 1) & \text{если } x < 0
	\end{cases}
	\end{equation}
	$$
	![ELU](ELU.png)
- **Преимущества**:
	- Уменьшение проблемы исчезающего градиента: Для отрицательных входных значений ELU имеет ненулевой градиент, что помогает уменьшить проблему исчезающего градиента, характерную для ReLU и Leaky ReLU.
	- Центрирование выходных значений вокруг нуля: Это свойство может улучшить скорость обучения, так как средние активации ближе к нулю, в отличие от ReLU, которая всегда имеет неотрицательный выход.
	- **Гладкая функция**: ELU является более гладкой функцией, особенно вокруг нуля, что может способствовать более стабильному обучению.
	- **Адаптивность**: Параметр _**a**_ в ELU позволяет адаптировать функцию активации к конкретным требованиям задачи, что может быть полезно в различных приложениях.

Эти преимущества делают ELU интересным выбором для многих архитектур нейронных сетей, особенно там, где проблемы исчезающего или взрывающегося градиента могут быть критичными. Тем не менее, стоит учитывать, что использование ELU может привести к несколько большим вычислительным затратам по сравнению с ReLU из-за экспоненциальной операции в её формуле.

Несмотря на некоторые недостатки, такие как увеличенные вычислительные затраты, ELU предоставляет полезные свойства, которые могут значительно улучшить производительность глубоких нейронных сетей, особенно в задачах, требующих более тщательного управления распространением градиентов в процессе обучения.
### Типы задач
- **Глубокие нейронные сети**: В глубоких сетях, где проблема исчезающего градиента может быть критичной, ELU может помочь улучшить обучение и сходимость, благодаря своей способности передавать отрицательные значения.
- **Задачи с комплексными моделями**: В случаях, когда архитектура модели сложная и требует тонкой настройки, ELU может предложить лучшую общую производительность за счет своей ненулевой производной для отрицательных входов.
- **Регрессия и прогнозирование временных рядов**: ELU может быть полезна в задачах регрессии и прогнозирования временных рядов, где данные могут иметь широкий диапазон и требуют аккуратной обработки отрицательных значений.
- **Обработка изображений и компьютерное зрение**: В этих областях ELU может помочь в обучении более глубоких и эффективных моделей, особенно когда необходимо избежать исчезающего градиента.
## **Softmax**

Функция активации Softmax широко используется в нейронных сетях, особенно в контексте задач классификации. Она преобразует вектор вещественных чисел (логиты, полученные из предыдущего слоя нейронной сети) в вероятностное распределение.

- **Формула**: $$\text{Softmax}(x)_i=\frac{e^{x_i}}{\sum_{j=1}^{K}e^{x_j}}
$$
	Где:
		$x$ - входной вектор (логиты)
		$x_i$ - $i$-ый элемент входного вектора
		$K$ - кол-во классов или длинна входного вектора
		$e$ - экспонента
### Принцип работы
Softmax преобразует каждый элемент входного вектора в число от 0 до 1, и все эти выходные значения в сумме дают 1. Таким образом, каждое значение интерпретируется как вероятность принадлежности к соответствующему классу.
### Типы задач
Softmax является стандартным выбором для последнего слоя в нейронных сетях, решающих задачи классификации, такие как:
- **Распознавание изображений**: Идентификация объектов или сущностей на изображениях, где каждый класс соответствует определенному объекту или категории.
- **Обработка естественного языка**: Классификация текстов по темам, определение тональности текста или распознавание намерений в диалоговых системах.
- **Медицинская диагностика**: Классификация медицинских изображений для диагностики различных заболеваний.

В каждом из этих случаев Softmax позволяет модели выражать уверенность в отношении каждого возможного класса, что делает её идеальным выбором для задач, где требуется не просто определить наиболее вероятный класс, но и оценить вероятности всех потенциальных классов.
## Скорость выполнения
ReLU обычно вычисляется значительно быстрее, чем сигмоида и гиперболический тангенс. Это связано с простотой и эффективностью операции ReLU. Подробнее о причинах быстродействия ReLU:

- Простота вычисления: ReLU имеет очень простую формулу и не включает в себя дорогостоящих операций, таких как возведение в экспоненту.
- Отсутствие вычисления экспоненты: В отличие от сигмоиды и гиперболического тангенса, ReLU не требует вычисления экспоненты, что делает ее более эффективной в вычислениях.
- Способствует разреженности активаций: ReLU часто приводит к разреженным активациям, уменьшая количество операций при прямом проходе через сеть.

Точное численное сравнение скорости выполнения зависит от многих факторов, но обычно ReLU значительно быстрее. Например, по результатам экспериментов, ReLU может быть до нескольких порядков быстрее, чем сигмоида и гиперболический тангенс.

# Заключение

Выбор подходящей функции активации является ключевым решением при проектировании нейронных сетей. Различные функции активации имеют свои особенности, которые влияют на производительность и эффективность модели:

**Sigmoid и Tanh**:
- Хорошо подходят для простых сетей и задач, где требуется строгий диапазон выходных значений (0-1 для Sigmoid, -1 до 1 для Tanh).
- Имеют проблему исчезающего градиента, что затрудняет их использование в глубоких сетях.

**ReLU**:
- Эффективна в глубоких нейронных сетях благодаря быстрому вычислению и решению проблемы исчезающего градиента при положительных значениях.
- Может страдать от "мертвых нейронов" при отрицательных входных значениях.

**Leaky ReLU и PReLU**:
- Предложены для решения проблемы "мертвых нейронов" в ReLU.
- Leaky ReLU имеет фиксированный небольшой градиент для отрицательных значений, в то время как PReLU имеет обучаемый параметр, что делает её более гибкой.

**ELU**:
- Улучшает общие характеристики ReLU, предоставляя ненулевой градиент для отрицательных входных значений.
- Может способствовать более быстрому обучению за счет центрирования выходных значений вокруг нуля.

**Softmax**:
- Идеально подходит для задач классификации в выходном слое, так как преобразует логиты в вероятности классов.